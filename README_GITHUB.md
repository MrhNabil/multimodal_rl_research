# Multimodal-RL-VQA

> **Compositional Skill Learning in Multimodal Reinforcement Learning for Visual Question Answering**

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org)
[![CLIP](https://img.shields.io/badge/CLIP-ViT--B%2F32-green.svg)](https://github.com/openai/CLIP)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## üìã Overview

This repository investigates whether **reinforcement learning can compose frozen vision skills with trainable language skills** for Visual Question Answering (VQA), inspired by the paper *"From f(x) and g(x) to f(g(x)): LLMs Learn New Skills in RL by Composing Old Ones"*.

### Key Research Question

> Can RL compose pretrained visual features (CLIP) with trainable classification heads to answer visual questions without intermediate supervision?

### Key Results

| Method | Accuracy | Details |
|--------|----------|---------|
| **Supervised Learning** | **74.0%** | Cross-entropy loss, 1000 steps |
| RL (REINFORCE) | 53.7% | Policy gradient, 3000 steps |
| Frozen Baseline | 0.2% | No training |

**Finding**: Supervised learning outperforms RL by 20+ percentage points on this multimodal task, suggesting cross-modal skill composition is more challenging than within-text composition.

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Image     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  CLIP ViT    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Projection ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Fusion     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂ Answer
‚îÇ  224√ó224    ‚îÇ    ‚îÇ  (FROZEN)    ‚îÇ    ‚îÇ    MLP     ‚îÇ    ‚îÇ   Layer     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ   151M       ‚îÇ    ‚îÇ (trainable)‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚ñ≤
                                                               ‚îÇ
                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ  Question  ‚îÇ
              ‚îÇ   Type     ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

- **Vision Encoder**: CLIP ViT-B/32 (frozen, 151M parameters)
- **Projection Layer**: MLP (trainable, ~500K parameters)
- **Classification Heads**: 4 type-specific heads (color, shape, count, spatial)

---

## üìä Experiments Summary

I conducted **61+ experiments** across:

### Training Methods
- Frozen baseline (no training)
- Supervised learning (cross-entropy)
- Reinforcement learning (REINFORCE)

### Learning Rate Sensitivity
| Learning Rate | RL Accuracy |
|---------------|------------|
| 1e-5 | 29.4% |
| 1e-4 | 45.2% |
| **2e-4** | **53.7%** (optimal) |
| 1e-3 | 29.3% |
| 1e-2 | 14.2% |

### Per-Question-Type Performance
| Type | Supervised | RL |
|------|------------|-----|
| Count | 82.0% | 58.0% |
| Shape | 77.4% | 71.8% |
| Color | 75.7% | **20.6%** |
| Spatial | 61.3% | 39.8% |

**Key Finding**: RL struggles severely with color questions (20.6%) but performs well on shape (71.8%).

---

## üìÅ Repository Structure

```
multimodal-rl-vqa/
‚îú‚îÄ‚îÄ README.md                    # This file
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ LICENSE                      # MIT License
‚îÇ
‚îú‚îÄ‚îÄ src/                         # Source code
‚îÇ   ‚îú‚îÄ‚îÄ models/                  # Model architectures
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vqa_model.py         # Main VQA model
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clip_encoder.py      # CLIP wrapper
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ classification_heads.py
‚îÇ   ‚îú‚îÄ‚îÄ training/                # Training logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ supervised.py        # Supervised training
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ reinforce.py         # REINFORCE algorithm
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trainer.py           # Main trainer class
‚îÇ   ‚îú‚îÄ‚îÄ data/                    # Data handling
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataset.py           # VQA dataset class
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocessing.py     # Data preprocessing
‚îÇ   ‚îî‚îÄ‚îÄ utils/                   # Utilities
‚îÇ       ‚îú‚îÄ‚îÄ metrics.py           # Evaluation metrics
‚îÇ       ‚îî‚îÄ‚îÄ visualization.py     # Plotting functions
‚îÇ
‚îú‚îÄ‚îÄ configs/                     # Experiment configurations
‚îÇ   ‚îú‚îÄ‚îÄ supervised.yaml          # Supervised training config
‚îÇ   ‚îî‚îÄ‚îÄ reinforce.yaml           # RL training config
‚îÇ
‚îú‚îÄ‚îÄ scripts/                     # Runnable scripts
‚îÇ   ‚îú‚îÄ‚îÄ train.py                 # Main training script
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py              # Evaluation script
‚îÇ   ‚îî‚îÄ‚îÄ prepare_data.py          # Data preparation
‚îÇ
‚îú‚îÄ‚îÄ experiments/                 # Experiment results
‚îÇ   ‚îú‚îÄ‚îÄ results/                 # Raw results (JSON)
‚îÇ   ‚îî‚îÄ‚îÄ figures/                 # Generated plots
‚îÇ
‚îú‚îÄ‚îÄ docs/                        # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ IEEE_PROJECT_REPORT.pdf  # Full IEEE paper
‚îÇ   ‚îî‚îÄ‚îÄ IEEE_PROJECT_REPORT.tex  # LaTeX source
‚îÇ
‚îî‚îÄ‚îÄ notebooks/                   # Jupyter notebooks
    ‚îî‚îÄ‚îÄ analysis.ipynb           # Results analysis
```

---

## üöÄ Quick Start

### 1. Installation

```bash
git clone https://github.com/yourusername/multimodal-rl-vqa.git
cd multimodal-rl-vqa
pip install -r requirements.txt
```

### 2. Prepare Data

```bash
python scripts/prepare_data.py --num_samples 5000
```

### 3. Train Models

**Supervised Learning:**
```bash
python scripts/train.py --method supervised --lr 0.0002 --steps 1000
```

**Reinforcement Learning:**
```bash
python scripts/train.py --method reinforce --lr 0.0002 --steps 3000
```

### 4. Evaluate

```bash
python scripts/evaluate.py --checkpoint experiments/checkpoints/best_model.pt
```

---

## üìà Results Visualization

### Method Comparison
![Method Comparison](experiments/figures/method_comparison.png)

### Learning Rate Sensitivity
![LR Sensitivity](experiments/figures/learning_rate.png)

### Per-Type Accuracy
![Per-Type](experiments/figures/per_type.png)

---

## üìÑ Citation

If you use this code in your research, please cite:

```bibtex
@misc{nabil2024multimodal,
  author = {Rakib Hossain Nabil},
  title = {Compositional Skill Learning in Multimodal Reinforcement Learning for Visual Question Answering},
  year = {2024},
  institution = {North South University},
  url = {https://github.com/yourusername/multimodal-rl-vqa}
}
```

---

## üîë Key Findings

1. **Supervised > RL**: 74.0% vs 53.7% accuracy
2. **Optimal LR for RL**: 2e-4 (too high causes collapse)
3. **Skill-specific composability**: Shape (72%) composes better than color (21%) via RL
4. **More data doesn't help**: 50K samples gave 61.5% (worse than 5K with 68.7%)
5. **Frozen features limit spatial reasoning**: 24-61% on spatial questions

---

## üìö References

1. Anonymous. "From f(x) and g(x) to f(g(x)): LLMs Learn New Skills in RL." arXiv:2509.25123, 2024.
2. Radford et al. "Learning Transferable Visual Models From Natural Language Supervision." ICML, 2021.
3. Williams. "Simple Statistical Gradient-Following Algorithms for RL." Machine Learning, 1992.
4. Antol et al. "VQA: Visual Question Answering." ICCV, 2015.
5. Johnson et al. "CLEVR: A Diagnostic Dataset for Compositional Reasoning." CVPR, 2017.

---

## üìù License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üë§ Author

**Rakib Hossain Nabil**  
Department of Electrical and Computer Engineering  
North South University, Dhaka, Bangladesh  
ID: 2131005642

---

## üôè Acknowledgments

- OpenAI for the CLIP model
- The authors of the compositional learning paper for inspiration
- North South University for support
